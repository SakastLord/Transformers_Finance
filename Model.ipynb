{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random as rand\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check the integrity of the data\n",
    "\n",
    "data = pd.read_pickle('final_data')\n",
    "\n",
    "print('Number of Nan in data : ' + str(data.isna().sum().sum()))\n",
    "\n",
    "num_zeros = 0\n",
    "weird_dim = 0\n",
    "for i in data.index:\n",
    "    for j in data.columns:\n",
    "        num_zeros += np.count_nonzero(data.loc[i,j][0:4]==0)\n",
    "        if len(data.iloc[0,0])!=10:\n",
    "            weird_dim += 1\n",
    "print('Number of unwanted 0 in data : ' + str(num_zeros))\n",
    "print('Number of data with weird sizes : ' + str(weird_dim))\n",
    "\n",
    "print('Shape of data : ' + str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preview of the raw data\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Handle a slice of dataframe: turn it in numpy array and add classification tokens'''        \n",
    "def toNumpy(sliced):             \n",
    "        \n",
    "        data_list = []\n",
    "        \n",
    "        for i in range(len(sliced.index)):\n",
    "            data_list.append(sliced.iloc[i].tolist())\n",
    "\n",
    "        return np.array(data_list)\n",
    "    \n",
    "\n",
    "'''Allow to get a slice of size num_row from dataframe data, return the slice as numpy array '''\n",
    "def getSlice(data, num_row, start_index):\n",
    "        \n",
    "        if num_row >= data.shape[0]:\n",
    "            raise Exception('num_row must not be greater than the number of row in data : ' + str(data.shape[0]))\n",
    "        if num_row<=0:\n",
    "            raise Exception('no 0 for num_row plz')\n",
    "        \n",
    "        \n",
    "        sliced = data.iloc[start_index:start_index+num_row]\n",
    "        return toNumpy(sliced)\n",
    "        \n",
    "        \n",
    "''' add positional and stock encoding '''\n",
    "def addEncoding(data_list, num_row):       \n",
    "        if data_list.shape != (num_row,477,11):\n",
    "            raise Exception('data_list does not have the right shape : ' + str(data_list.shape))\n",
    "        \n",
    "        for i in range(data_list.shape[0]):\n",
    "            for j in range(data_list.shape[1]):\n",
    "                \n",
    "                if data_list[i,j,9] != -1.111:\n",
    "                    raise Exception('the positional encoding is already set somehow?')\n",
    "                \n",
    "                data_list[i,j,9]= (i+1)/num_row\n",
    "                \n",
    "                if data_list[i,j,10] != -2.222:\n",
    "                    raise Exception('the stock encoding is already set somehow?')\n",
    "                \n",
    "                data_list[i,j,10]= (j+1)/477\n",
    "        \n",
    "        return data_list\n",
    "    \n",
    "\n",
    "''' add the 477 empty arrays to be used as classification tokens '''\n",
    "def addClassificationTokens(data_list):\n",
    "    return np.concatenate((np.zeros((1,477,11)), data_list), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''handle dataframe and returns the expected results for 5 days regression'''\n",
    "def getRegResults(data, start, num_days):\n",
    "    \n",
    "    sliced = toNumpy(data.iloc[start:start+(num_days*10)])\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for s in range(sliced.shape[1]):\n",
    "        \n",
    "        stock = []\n",
    "        col = sliced[:,s,:]\n",
    "\n",
    "        i=0\n",
    "        while( len(stock)<num_days ):\n",
    "\n",
    "            if col[i][6]==1:\n",
    "                #info = col[i,[0,1,2,3,4,5,8]]\n",
    "                daily_change = (col[i,3]/col[i,2]) - 1\n",
    "                #info[6]=daily_change\n",
    "                stock.append([daily_change])\n",
    "            i+=1\n",
    "        \n",
    "        results.append(stock)\n",
    "    \n",
    "    return np.transpose(np.array(results), (1,0,2))\n",
    "\n",
    "\n",
    "'''given a sequence of daily variation, define the class of a sequence'''\n",
    "def getClass(array):\n",
    "    #var = np.var(array, dtype=np.float64)\n",
    "    mean = array.mean()\n",
    "    #if var>0.01:\n",
    "    #    return 3 #volatile\n",
    "    if mean>0.003:\n",
    "        return 0 #bullish\n",
    "    elif mean<-0.003:\n",
    "        return 1 #bearish\n",
    "    else:\n",
    "        return 2 #stable\n",
    "    \n",
    "    \n",
    "    \n",
    "'''handle dataframe and returns the expected results for classification'''\n",
    "#cls is done by looking at 3 days and classifying into : volatile, stable, bearish, bullish.\n",
    "\n",
    "def getClsResults(data, num_days):\n",
    "    if num_days>data.shape[0]:\n",
    "        raise Exception('the regression must be done on more days than the classification')\n",
    "    \n",
    "    sliced = data[0:num_days,:,0]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(sliced.shape[1]):\n",
    "        #get info on 3 days\n",
    "        stock = sliced[:,i]\n",
    "        cls = getClass(stock)\n",
    "        results.append(cls)\n",
    "        \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 477, 1)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "a = getRegResults(data, 0,5)\n",
    "print(a.shape)\n",
    "print(a.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(477,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = getClsResults(a, 3)\n",
    "print(b.dtype)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "''' reshape the (date, stock, features) into (date*stock, feature)'''\n",
    "def flatten(sliced):\n",
    "    return sliced.reshape(sliced.shape[0]*sliced.shape[1], sliced.shape[2])\n",
    "\n",
    "\n",
    "'''generate a numpy batch of num_batch samples for num_row dates and for all stocks, use without start param'''\n",
    "def getBatches(data, num_row, num_batch, start = -1):\n",
    "    \n",
    "    batches = []\n",
    "    results_reg = []\n",
    "    results_cls = []\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        \n",
    "        if start == -1:\n",
    "            start_index = rand.randint(0, data.shape[0] - num_row-5)\n",
    "        else:\n",
    "            start_index = start\n",
    "        #print(start_index)\n",
    "        \n",
    "        batch = getSlice(data, num_row,start_index) \n",
    "        result_reg = getRegResults(data, start_index+num_row, 5)\n",
    "        result_cls = getClsResults(result_reg,3)\n",
    "        \n",
    "        batches.append(flatten(addClassificationTokens(addEncoding(batch, num_row))))\n",
    "        results_reg.append(flatten(result_reg))\n",
    "        results_cls.append(result_cls)\n",
    "    \n",
    "    #try:\n",
    "    tensor_batch=torch.Tensor(np.transpose(np.array(batches),(1,0,2)))\n",
    "    tensor_reg=torch.Tensor(np.transpose(np.array(results_reg), (1,0,2)))\n",
    "    tensor_cls=torch.Tensor(np.transpose(np.array(results_cls), (1,0))).long()\n",
    "    \n",
    "    return tensor_batch, tensor_reg, tensor_cls\n",
    "    #except:\n",
    "    #    print(np.array(batches).shape)\n",
    "    #    print(np.array(results_reg).shape)\n",
    "    #    print(np.array(results_cls).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = data.iloc[60:60+3,305]\n",
    "print(t.name)\n",
    "mean = []\n",
    "for i in t.index:\n",
    "    mean.append(t[i][3]/t[i][2] - 1)\n",
    "    print(str(t[i][6]==1) + '    ' + str(t[i][3]/t[i][2] - 1))\n",
    "print(mean)\n",
    "print(np.mean(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = getBatches(data, 10,2, 50)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b.dtype)\n",
    "print(b.shape)\n",
    "print(b[477*2+305].numpy())\n",
    "\n",
    "print(c.dtype)\n",
    "print(c.shape)\n",
    "c[305].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'order of the embedding : \\n0 : high\\n1 : low\\n2 : open\\n3 : close\\n4 : volume\\n5 : adj close\\n6 : Real Or Fake Value?\\n7 : year Positional Encoding\\n8 : price change compared to yesterday\\n9 : positional encoding\\n10 : stock encoding\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('final_data')\n",
    "\n",
    "'''order of the embedding : \n",
    "0 : high\n",
    "1 : low\n",
    "2 : open\n",
    "3 : close\n",
    "4 : volume\n",
    "5 : adj close\n",
    "6 : Real Or Fake Value?\n",
    "7 : year Positional Encoding\n",
    "8 : price change compared to yesterday\n",
    "9 : positional encoding\n",
    "10 : stock encoding\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^TNX</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>A</th>\n",
       "      <th>AAP</th>\n",
       "      <th>AAPL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-08-30</th>\n",
       "      <td>[4.2230000495910645, 4.188000202178955, 4.2230...</td>\n",
       "      <td>[15.579999923706055, 15.300000190734865, 15.35...</td>\n",
       "      <td>[15.40772533416748, 15.064377784729006, 15.379...</td>\n",
       "      <td>[25.13333320617676, 24.79999923706055, 24.9133...</td>\n",
       "      <td>[2.4800000190734863, 2.4257142543792725, 2.428...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-08-31</th>\n",
       "      <td>[4.184000015258788, 4.0960001945495605, 4.1820...</td>\n",
       "      <td>[15.850000381469727, 15.279999732971193, 15.64...</td>\n",
       "      <td>[15.021459579467773, 14.320457458496096, 15.0,...</td>\n",
       "      <td>[24.979999542236328, 24.6200008392334, 24.9799...</td>\n",
       "      <td>[2.4964284896850586, 2.4285714626312256, 2.433...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-01</th>\n",
       "      <td>[4.1599998474121085, 4.081999778747559, 4.0939...</td>\n",
       "      <td>[15.390000343322756, 14.720000267028807, 15.39...</td>\n",
       "      <td>[14.98569393157959, 14.470672607421875, 14.613...</td>\n",
       "      <td>[24.913333892822266, 24.63999938964844, 24.706...</td>\n",
       "      <td>[2.570714235305786, 2.442142963409424, 2.45000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-02</th>\n",
       "      <td>[4.196000099182129, 4.116000175476073, 4.13199...</td>\n",
       "      <td>[15.050000190734865, 14.18000030517578, 14.970...</td>\n",
       "      <td>[15.236051559448242, 14.570815086364744, 14.80...</td>\n",
       "      <td>[24.893333435058594, 24.64666748046875, 24.893...</td>\n",
       "      <td>[2.557857036590576, 2.4878571033477783, 2.5357...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-03</th>\n",
       "      <td>[4.297999858856201, 4.165999889373778, 4.18200...</td>\n",
       "      <td>[14.380000114440918, 13.789999961853027, 14.35...</td>\n",
       "      <td>[15.135908126831055, 14.484978675842285, 15.12...</td>\n",
       "      <td>[25.02666664123535, 24.65999984741211, 24.7399...</td>\n",
       "      <td>[2.5657143592834477, 2.5007143020629883, 2.500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-05</th>\n",
       "      <td>[4.297999858856201, 4.165999889373778, 4.18200...</td>\n",
       "      <td>[14.380000114440918, 13.789999961853027, 14.35...</td>\n",
       "      <td>[15.135908126831055, 14.484978675842285, 15.12...</td>\n",
       "      <td>[25.02666664123535, 24.65999984741211, 24.7399...</td>\n",
       "      <td>[2.5657143592834477, 2.5007143020629883, 2.500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-06</th>\n",
       "      <td>[4.297999858856201, 4.165999889373778, 4.18200...</td>\n",
       "      <td>[14.380000114440918, 13.789999961853027, 14.35...</td>\n",
       "      <td>[15.135908126831055, 14.484978675842285, 15.12...</td>\n",
       "      <td>[25.02666664123535, 24.65999984741211, 24.7399...</td>\n",
       "      <td>[2.5657143592834477, 2.5007143020629883, 2.500...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-07</th>\n",
       "      <td>[4.293000221252441, 4.244999885559082, 4.28100...</td>\n",
       "      <td>[14.640000343322756, 14.029999732971193, 14.52...</td>\n",
       "      <td>[14.97138786315918, 14.692418098449707, 14.771...</td>\n",
       "      <td>[25.29333305358887, 24.96666717529297, 24.9666...</td>\n",
       "      <td>[2.5850000381469727, 2.5164284706115723, 2.528...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-08</th>\n",
       "      <td>[4.287000179290772, 4.1550002098083505, 4.2649...</td>\n",
       "      <td>[14.3100004196167, 13.880000114440918, 14.1599...</td>\n",
       "      <td>[14.66380500793457, 14.406294822692873, 14.542...</td>\n",
       "      <td>[25.26666641235352, 25.03333282470703, 25.0933...</td>\n",
       "      <td>[2.61214280128479, 2.548571348190308, 2.549999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-09</th>\n",
       "      <td>[4.203999996185304, 4.13100004196167, 4.133999...</td>\n",
       "      <td>[14.40999984741211, 13.699999809265135, 14.119...</td>\n",
       "      <td>[15.22889804840088, 14.53505039215088, 14.5493...</td>\n",
       "      <td>[25.20000076293945, 24.479999542236328, 25.066...</td>\n",
       "      <td>[2.5928571224212646, 2.5199999809265137, 2.578...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         ^TNX  \\\n",
       "2004-08-30  [4.2230000495910645, 4.188000202178955, 4.2230...   \n",
       "2004-08-31  [4.184000015258788, 4.0960001945495605, 4.1820...   \n",
       "2004-09-01  [4.1599998474121085, 4.081999778747559, 4.0939...   \n",
       "2004-09-02  [4.196000099182129, 4.116000175476073, 4.13199...   \n",
       "2004-09-03  [4.297999858856201, 4.165999889373778, 4.18200...   \n",
       "2004-09-05  [4.297999858856201, 4.165999889373778, 4.18200...   \n",
       "2004-09-06  [4.297999858856201, 4.165999889373778, 4.18200...   \n",
       "2004-09-07  [4.293000221252441, 4.244999885559082, 4.28100...   \n",
       "2004-09-08  [4.287000179290772, 4.1550002098083505, 4.2649...   \n",
       "2004-09-09  [4.203999996185304, 4.13100004196167, 4.133999...   \n",
       "\n",
       "                                                         ^VIX  \\\n",
       "2004-08-30  [15.579999923706055, 15.300000190734865, 15.35...   \n",
       "2004-08-31  [15.850000381469727, 15.279999732971193, 15.64...   \n",
       "2004-09-01  [15.390000343322756, 14.720000267028807, 15.39...   \n",
       "2004-09-02  [15.050000190734865, 14.18000030517578, 14.970...   \n",
       "2004-09-03  [14.380000114440918, 13.789999961853027, 14.35...   \n",
       "2004-09-05  [14.380000114440918, 13.789999961853027, 14.35...   \n",
       "2004-09-06  [14.380000114440918, 13.789999961853027, 14.35...   \n",
       "2004-09-07  [14.640000343322756, 14.029999732971193, 14.52...   \n",
       "2004-09-08  [14.3100004196167, 13.880000114440918, 14.1599...   \n",
       "2004-09-09  [14.40999984741211, 13.699999809265135, 14.119...   \n",
       "\n",
       "                                                            A  \\\n",
       "2004-08-30  [15.40772533416748, 15.064377784729006, 15.379...   \n",
       "2004-08-31  [15.021459579467773, 14.320457458496096, 15.0,...   \n",
       "2004-09-01  [14.98569393157959, 14.470672607421875, 14.613...   \n",
       "2004-09-02  [15.236051559448242, 14.570815086364744, 14.80...   \n",
       "2004-09-03  [15.135908126831055, 14.484978675842285, 15.12...   \n",
       "2004-09-05  [15.135908126831055, 14.484978675842285, 15.12...   \n",
       "2004-09-06  [15.135908126831055, 14.484978675842285, 15.12...   \n",
       "2004-09-07  [14.97138786315918, 14.692418098449707, 14.771...   \n",
       "2004-09-08  [14.66380500793457, 14.406294822692873, 14.542...   \n",
       "2004-09-09  [15.22889804840088, 14.53505039215088, 14.5493...   \n",
       "\n",
       "                                                          AAP  \\\n",
       "2004-08-30  [25.13333320617676, 24.79999923706055, 24.9133...   \n",
       "2004-08-31  [24.979999542236328, 24.6200008392334, 24.9799...   \n",
       "2004-09-01  [24.913333892822266, 24.63999938964844, 24.706...   \n",
       "2004-09-02  [24.893333435058594, 24.64666748046875, 24.893...   \n",
       "2004-09-03  [25.02666664123535, 24.65999984741211, 24.7399...   \n",
       "2004-09-05  [25.02666664123535, 24.65999984741211, 24.7399...   \n",
       "2004-09-06  [25.02666664123535, 24.65999984741211, 24.7399...   \n",
       "2004-09-07  [25.29333305358887, 24.96666717529297, 24.9666...   \n",
       "2004-09-08  [25.26666641235352, 25.03333282470703, 25.0933...   \n",
       "2004-09-09  [25.20000076293945, 24.479999542236328, 25.066...   \n",
       "\n",
       "                                                         AAPL  \n",
       "2004-08-30  [2.4800000190734863, 2.4257142543792725, 2.428...  \n",
       "2004-08-31  [2.4964284896850586, 2.4285714626312256, 2.433...  \n",
       "2004-09-01  [2.570714235305786, 2.442142963409424, 2.45000...  \n",
       "2004-09-02  [2.557857036590576, 2.4878571033477783, 2.5357...  \n",
       "2004-09-03  [2.5657143592834477, 2.5007143020629883, 2.500...  \n",
       "2004-09-05  [2.5657143592834477, 2.5007143020629883, 2.500...  \n",
       "2004-09-06  [2.5657143592834477, 2.5007143020629883, 2.500...  \n",
       "2004-09-07  [2.5850000381469727, 2.5164284706115723, 2.528...  \n",
       "2004-09-08  [2.61214280128479, 2.548571348190308, 2.549999...  \n",
       "2004-09-09  [2.5928571224212646, 2.5199999809265137, 2.578...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0:10,45:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "getSlice works fine - tested and compared with the dataframe directly => its good\n",
    "time to make model\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, num_heads, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_size, num_heads=num_heads , dropout=dropout_rate)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.norm0 = nn.LayerNorm(input_size, eps=1e-6)\n",
    "        \n",
    "        self.ffn = FeedForwardNetwork(input_size, input_size*4 ,input_size, dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(input_size, eps=1e-6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            \n",
    "        #possibility here : add q,k,v linear layers for each stock independantly?\n",
    "            \n",
    "        y, wei = self.attention(x,x,x)\n",
    "        #print('0')\n",
    "        y = self.dropout(y)\n",
    "    \n",
    "        y = y+x\n",
    "        y = self.norm0(y)\n",
    "            \n",
    "        z = self.ffn(y)\n",
    "            \n",
    "        z = z+y\n",
    "        z = self.norm1(z)\n",
    "            \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, input_size, encoder_size, dropout_rate ):\n",
    "        super(EmbeddingLayer,self).__init__()\n",
    "        \n",
    "        self.embed = nn.Sequential(\n",
    "            nn.Linear(input_size, int((input_size+encoder_size)/1.5)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(int((input_size+encoder_size)/1.5), encoder_size*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.LayerNorm(encoder_size*2, eps=1e-6),\n",
    "            nn.Linear(encoder_size*2, encoder_size*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(encoder_size*2, encoder_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationLayer(nn.Module):\n",
    "    def __init__(self, encoder_size, number_classes, dropout_rate):\n",
    "        super(ClassificationLayer, self).__init__()\n",
    "        \n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(encoder_size, encoder_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(encoder_size, encoder_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(encoder_size, number_classes),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.cls(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionLayer(nn.Module):\n",
    "    def __init__(self, encoder_size, dropout_rate):\n",
    "        super(RegressionLayer, self).__init__()\n",
    "        \n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(encoder_size, encoder_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(encoder_size, encoder_size),\n",
    "            nn.Softsign(),\n",
    "            nn.Linear(encoder_size, 1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.reg(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, encoder_size, num_encoders, num_heads, dropout_rate ):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.embed = EmbeddingLayer(input_size, encoder_size, dropout_rate)\n",
    "        \n",
    "        self.norm0 = nn.LayerNorm(encoder_size, eps=1e-6)\n",
    "        \n",
    "        encoders = [EncoderLayer(encoder_size, num_heads, dropout_rate)\n",
    "                    for _ in range(num_encoders)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "        \n",
    "        self.cls_layer = ClassificationLayer(encoder_size,3, 0.1)\n",
    "        self.reg_layer = RegressionLayer(64,0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "            \n",
    "        y = self.embed(x)\n",
    "        y = self.norm0(y)\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            y = layer(y)\n",
    "        \n",
    "        cls = self.cls_layer(y.narrow(0,0,477))\n",
    "        reg = self.reg_layer(y.narrow(0,477,477*5))\n",
    "        \n",
    "        return reg, cls\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Training procedure'''\n",
    "\n",
    "model = Encoder(input_size=11, encoder_size=64, num_encoders=3, num_heads=4, dropout_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embed): EmbeddingLayer(\n",
      "    (embed): Sequential(\n",
      "      (0): Linear(in_features=11, out_features=50, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=50, out_features=128, bias=True)\n",
      "      (4): GELU()\n",
      "      (5): Dropout(p=0.1, inplace=False)\n",
      "      (6): LayerNorm((128,), eps=1e-06, elementwise_affine=True)\n",
      "      (7): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (8): GELU()\n",
      "      (9): Dropout(p=0.1, inplace=False)\n",
      "      (10): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (11): GELU()\n",
      "      (12): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (norm0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): EncoderLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (norm0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (seq): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (1): EncoderLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (norm0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (seq): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "    (2): EncoderLayer(\n",
      "      (attention): MultiheadAttention(\n",
      "        (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (norm0): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "      (ffn): FeedForwardNetwork(\n",
      "        (seq): Sequential(\n",
      "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (1): GELU()\n",
      "          (2): Dropout(p=0.1, inplace=False)\n",
      "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (4): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((64,), eps=1e-06, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (cls_layer): ClassificationLayer(\n",
      "    (cls): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (4): GELU()\n",
      "      (5): Linear(in_features=64, out_features=3, bias=True)\n",
      "      (6): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (reg_layer): RegressionLayer(\n",
      "    (reg): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (4): Softsign()\n",
      "      (5): Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199132\n",
      "199132\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(total_params)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4293, 2, 11])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b,true_reg,true_cls = getBatches(data, num_row=8, num_batch=2)\n",
    "\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "pred_reg, pred_cls = model(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2385, 2, 1])True\n",
      "torch.Size([2385, 2, 1])False\n",
      "torch.Size([477, 2, 3])True\n",
      "torch.Size([477, 2])False\n"
     ]
    }
   ],
   "source": [
    "print(str(pred_reg.shape) + str(pred_reg.requires_grad))\n",
    "print(str(true_reg.shape)+ str(true_reg.requires_grad))\n",
    "print(str(pred_cls.shape) + str(pred_cls.requires_grad))\n",
    "print(str(true_cls.shape)+ str(true_cls.requires_grad))\n",
    "assert pred_reg.shape==true_reg.shape\n",
    "assert (pred_cls.shape[0]==true_cls.shape[0] and pred_cls.shape[1]==true_cls.shape[1])\n",
    "assert pred_cls.requires_grad==pred_reg.requires_grad==True\n",
    "assert true_cls.requires_grad==true_reg.requires_grad==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1305, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_loss = nn.MSELoss()\n",
    "\n",
    "loss_output = reg_loss(pred_reg, true_reg)\n",
    "\n",
    "for i in range(pred_cls.shape[1]):\n",
    "    cls_loss = nn.CrossEntropyLoss()\n",
    "    cls_loss_output = cls_loss(pred_cls[:,i,:], true_cls[:,i])\n",
    "    loss_output = loss_output + (1/pred_cls.shape[1])*cls_loss_output\n",
    "\n",
    "loss_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0\n",
      "0\n",
      "0\n",
      "0\n",
      "step 1\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "num_step = 2\n",
    "\n",
    "loss_values = []\n",
    "cls_loss_values = []\n",
    "reg_loss_values = []\n",
    "\n",
    "reg_loss = nn.MSELoss()\n",
    "cls_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=0.001)\n",
    "\n",
    "for t in range(num_step):\n",
    "    print('step ' + str(t))\n",
    "    batch,true_reg,true_cls = getBatches(data, num_row=8, num_batch=2)\n",
    "    \n",
    "    # clears optimizer buffer and forward() the model\n",
    "    optimizer.zero_grad()\n",
    "    pred_reg, pred_cls = model(batch)\n",
    "    \n",
    "    assert pred_reg.shape==true_reg.shape\n",
    "    assert (pred_cls.shape[0]==true_cls.shape[0] and pred_cls.shape[1]==true_cls.shape[1])\n",
    "    assert pred_cls.requires_grad==pred_reg.requires_grad==True\n",
    "    assert true_cls.requires_grad==true_reg.requires_grad==False\n",
    "    \n",
    "    # Double Loss calculation\n",
    "    loss_output = reg_loss(pred_reg, true_reg)\n",
    "    reg_loss_values.append(loss_output.item())\n",
    "    \n",
    "    cls_loss_value_batch = []\n",
    "    for i in range(pred_cls.shape[1]):\n",
    "        cls_loss_output = cls_loss(pred_cls[:,i,:], true_cls[:,i])\n",
    "        cls_loss_value_batch.append(cls_loss_output.item())\n",
    "        loss_output = loss_output + (1/pred_cls.shape[1])*cls_loss_output\n",
    "    \n",
    "    cls_loss_values.append(np.mean(cls_loss_value_batch))\n",
    "    loss_values.append(loss_output.item())\n",
    "    \n",
    "    # computes gradient and optimizes weights \n",
    "    loss_output.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mti830]",
   "language": "python",
   "name": "conda-env-mti830-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
